/usr/bin/env bash /Users/spafka/Desktop/flink/spark/shell/submit-mesos-client.sh
/Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/JavaEWAH-0.3.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/RoaringBitmap-0.5.11.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/ST4-4.0.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/activation-1.1.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/aircompressor-0.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/antlr-2.7.7.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/antlr-runtime-3.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/antlr4-runtime-4.7.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/aopalliance-1.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/aopalliance-repackaged-2.4.0-b34.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/apache-log4j-extras-1.2.17.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/apacheds-i18n-2.0.0-M15.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/apacheds-kerberos-codec-2.0.0-M15.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/api-asn1-api-1.0.0-M20.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/api-util-1.0.0-M20.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/arpack_combined_all-0.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/arrow-format-0.8.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/arrow-memory-0.8.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/arrow-vector-0.8.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/avro-1.7.7.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/avro-ipc-1.7.7.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/avro-mapred-1.7.7-hadoop2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/base64-2.3.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/bcprov-jdk15on-1.58.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/bonecp-0.8.0.RELEASE.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/breeze-macros_2.11-0.13.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/breeze_2.11-0.13.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/calcite-avatica-1.2.0-incubating.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/calcite-core-1.2.0-incubating.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/calcite-linq4j-1.2.0-incubating.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/chill-java-0.8.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/chill_2.11-0.8.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-beanutils-1.7.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-beanutils-core-1.8.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-cli-1.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-codec-1.10.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-collections-3.2.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-compiler-3.0.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-compress-1.4.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-configuration-1.6.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-crypto-1.0.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-dbcp-1.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-digester-1.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-httpclient-3.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-io-2.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-lang-2.6.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-lang3-3.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-logging-1.1.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-math3-3.4.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-net-2.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/commons-pool-1.5.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/compress-lzf-1.0.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/core-1.1.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/curator-client-2.6.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/curator-framework-2.6.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/curator-recipes-2.6.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/datanucleus-api-jdo-3.2.6.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/datanucleus-core-3.2.10.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/datanucleus-rdbms-3.2.9.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/derby-10.12.1.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/eigenbase-properties-1.1.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/flatbuffers-1.2.0-3f79e055.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/gson-2.2.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/guava-14.0.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/guice-3.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/guice-servlet-3.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-annotations-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-auth-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-client-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-common-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-hdfs-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-mapreduce-client-app-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-mapreduce-client-common-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-mapreduce-client-core-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-yarn-api-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-yarn-client-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-yarn-common-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-yarn-server-common-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hadoop-yarn-server-web-proxy-2.6.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hive-exec-1.2.1.spark2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hive-metastore-1.2.1.spark2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hk2-api-2.4.0-b34.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hk2-locator-2.4.0-b34.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hk2-utils-2.4.0-b34.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/hppc-0.7.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/htrace-core-3.0.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/httpclient-4.5.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/httpcore-4.4.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/ivy-2.4.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-annotations-2.6.7.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-core-2.6.7.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-core-asl-1.9.13.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-databind-2.6.7.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-jaxrs-1.9.13.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-mapper-asl-1.9.13.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-module-paranamer-2.7.9.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-module-scala_2.11-2.6.7.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jackson-xc-1.9.13.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/janino-3.0.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/java-xmlbuilder-1.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javassist-3.18.1-GA.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javax.annotation-api-1.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javax.inject-1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javax.inject-2.4.0-b34.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javax.servlet-api-3.1.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javax.ws.rs-api-2.0.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/javolution-5.5.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jaxb-api-2.2.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jcl-over-slf4j-1.7.16.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jdo-api-3.0.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-client-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-common-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-container-servlet-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-container-servlet-core-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-guava-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-media-jaxb-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jersey-server-2.22.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jets3t-0.9.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-6.1.26.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-client-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-continuation-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-http-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-io-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-jndi-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-plus-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-proxy-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-security-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-server-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-servlet-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-servlets-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-util-6.1.26.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-util-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-webapp-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jetty-xml-9.3.20.v20170531.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/joda-time-2.9.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jodd-core-3.5.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/json4s-ast_2.11-3.5.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/json4s-core_2.11-3.5.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/json4s-jackson_2.11-3.5.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/json4s-scalap_2.11-3.5.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jsr305-1.3.9.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jta-1.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jtransforms-2.4.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/jul-to-slf4j-1.7.16.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/kryo-shaded-3.0.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/leveldbjni-all-1.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/libfb303-0.9.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/libthrift-0.9.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/log4j-1.2.17.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/lz4-java-1.4.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/machinist_2.11-0.6.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/macro-compat_2.11-1.1.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/mesos-1.4.0-shaded-protobuf.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/metrics-core-3.1.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/metrics-graphite-3.1.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/metrics-json-3.1.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/metrics-jvm-3.1.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/minlog-1.3.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/netty-3.9.9.Final.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/netty-all-4.1.17.Final.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/objenesis-2.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/opencsv-2.3.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/orc-core-1.4.1-nohive.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/orc-mapreduce-1.4.1-nohive.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/oro-2.0.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/osgi-resource-locator-1.0.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/paranamer-2.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-column-1.8.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-common-1.8.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-encoding-1.8.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-format-2.3.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-hadoop-1.8.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-hadoop-bundle-1.6.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/parquet-jackson-1.8.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/protobuf-java-2.5.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/py4j-0.10.6.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/pyrolite-4.13.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/scala-compiler-2.11.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/scala-library-2.11.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/scala-parser-combinators_2.11-1.0.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/scala-reflect-2.11.8.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/scala-xml_2.11-1.0.5.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/shapeless_2.11-2.3.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/slf4j-api-1.7.16.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/slf4j-log4j12-1.7.16.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/snappy-0.2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/snappy-java-1.1.7.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-catalyst_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-core_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-graphx_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-hive_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-kvstore_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-launcher_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-mesos_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-mllib-local_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-mllib_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-network-common_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-network-shuffle_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-repl_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-sketch_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-sql_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-streaming_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-tags_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-unsafe_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spark-yarn_2.11-2.4.0-SNAPSHOT.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spire-macros_2.11-0.13.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/spire_2.11-0.13.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/stax-api-1.0-2.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/stax-api-1.0.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/stream-2.7.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/stringtemplate-3.2.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/univocity-parsers-2.5.9.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/validation-api-1.1.0.Final.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/xbean-asm5-shaded-4.4.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/xercesImpl-2.9.1.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/xmlenc-0.52.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/xz-1.0.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/zookeeper-3.4.6.jar /Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/zstd-jni-1.3.2-2.jar
Spark Command: /Library/Java/JavaVirtualMachines/jdk1.8.0_151.jdk/Contents/Home/bin/java -cp /Users/spafka/Desktop/flink/spark/conf/:/Users/spafka/Desktop/flink/spark/assembly/target/scala-2.11/jars/* -Xmx1G org.apache.spark.deploy.SparkSubmit --master mesos://localhost:5050 --deploy-mode client --class org.apache.spark.examples.SparkWc --executor-memory 2G --total-executor-cores 2 /Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar
========================================
Using properties file: /Users/spafka/Desktop/flink/spark/conf/spark-defaults.conf
18/02/18 21:26:52 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en2)
18/02/18 21:26:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Parsed arguments:
  master                  mesos://localhost:5050
  deployMode              client
  executorMemory          2G
  executorCores           1
  totalExecutorCores      2
  propertiesFile          /Users/spafka/Desktop/flink/spark/conf/spark-defaults.conf
  driverMemory            1G
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               org.apache.spark.examples.SparkWc
  primaryResource         file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar
  name                    org.apache.spark.examples.SparkWc
  childArgs               []
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /Users/spafka/Desktop/flink/spark/conf/spark-defaults.conf:



18/02/18 21:26:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/18 21:26:53 INFO SparkSubmit: sparkSubmit ArrayBuffer(), ArrayBuffer(file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar), org.apache.spark.SparkConf@5b367418, org.apache.spark.examples.SparkWc, Parsed arguments:
  master                  mesos://localhost:5050
  deployMode              client
  executorMemory          2G
  executorCores           1
  totalExecutorCores      2
  propertiesFile          /Users/spafka/Desktop/flink/spark/conf/spark-defaults.conf
  driverMemory            1G
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               org.apache.spark.examples.SparkWc
  primaryResource         file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar
  name                    org.apache.spark.examples.SparkWc
  childArgs               []
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /Users/spafka/Desktop/flink/spark/conf/spark-defaults.conf:


    .verbose
Main class:
org.apache.spark.examples.SparkWc
Arguments:

Spark config:
(spark.jars,*********(redacted))
(spark.app.name,org.apache.spark.examples.SparkWc)
(spark.cores.max,2)
(spark.driver.memory,1G)
(spark.submit.deployMode,client)
(spark.master,mesos://localhost:5050)
(spark.executor.memory,2G)
Classpath elements:
file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar


18/02/18 21:26:53 INFO SparkContext: Running Spark version 2.4.0-SNAPSHOT
18/02/18 21:26:53 INFO SparkContext:
======================1=======================
Submitted application: Spark WC
18/02/18 21:26:53 INFO SparkContext: Spark configuration:
spark.app.name=Spark WC
spark.cores.max=2
spark.default.parallelism=8
spark.driver.memory=1G
spark.executor.memory=2G
spark.jars=file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar
spark.logConf=true
spark.master=mesos://localhost:5050
spark.submit.deployMode=client
18/02/18 21:26:53 INFO SparkEnv:  ==========================1.1 sc createSparkEnv =====================
conf => (spark.driver.host,172.20.10.4)|(spark.driver.memory,1G)|(spark.master,mesos://localhost:5050)|(spark.app.name,Spark WC)|(spark.default.parallelism,8)|(spark.driver.port,0)|(spark.cores.max,2)|(spark.executor.memory,2G)|(spark.jars,file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar)|(spark.executor.id,driver)|(spark.submit.deployMode,client)|(spark.logConf,true),
executorId => driver
bindAddress => 172.20.10.4,Some(0),
isLocal => false,
numUsableCores => 0
18/02/18 21:26:53 INFO SecurityManager: Changing view acls to: spafka
18/02/18 21:26:53 INFO SecurityManager: Changing modify acls to: spafka
18/02/18 21:26:53 INFO SecurityManager: Changing view acls groups to:
18/02/18 21:26:53 INFO SecurityManager: Changing modify acls groups to:
18/02/18 21:26:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spafka); groups with view permissions: Set(); users  with modify permissions: Set(spafka); groups with modify permissions: Set()
18/02/18 21:26:53 INFO SparkEnv:
==================1.2 Sc create RpcEnv ==================
systemName=> sparkDriver
bindAddress => 172.20.10.4
advertiseAddress=>172.20.10.4
numUsableCores=>0
!isDriver =>false

18/02/18 21:26:53 INFO NettyRpcEnvFactory: created JavaSerializerInstance org.apache.spark.serializer.JavaSerializerInstance@69653e16
18/02/18 21:26:53 INFO NettyRpcEnvFactory: create nettyEnv started
18/02/18 21:26:53 INFO NettyRpcEnvFactory: create nettyEnv finished NettyRpcEnv(transportConf=org.apache.spark.network.util.TransportConf@ea9b7c6,
 dispatcher=Dispatcher(endpoints={}, endpointRefs={}, receivers=[], stopped=false, threadpool=java.util.concurrent.ThreadPoolExecutor@e077866[Running, pool size = 8, active threads = 8, queued tasks = 0, completed tasks = 0], PoisonPill=org.apache.spark.rpc.netty.Dispatcher$EndpointData@ed3068a),
 streamManager=org.apache.spark.rpc.netty.NettyStreamManager@7c2b6087,
 transportContext=TransportContext{conf=org.apache.spark.network.util.TransportConf@ea9b7c6, rpcHandler=org.apache.spark.rpc.netty.NettyRpcHandler@3fffff43, closeIdleConnections=false},
 clientFactory=org.apache.spark.network.client.TransportClientFactory@a8e6492,
 fileDownloadFactory=null,
timeoutScheduler=java.util.concurrent.ScheduledThreadPoolExecutor@1c7fd41f[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0],
 clientConnectionExecutor=java.util.concurrent.ThreadPoolExecutor@3b77a04f[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0],
 server=null,
 stopped=false,
 outboxes={},
 conf=org.apache.spark.SparkConf@7b324585)
18/02/18 21:26:53 INFO NettyRpcEnv: Server 模式  start Server  at point 0
18/02/18 21:26:53 INFO TransportServer: 初始化TransPortserver ...
18/02/18 21:26:53 INFO TransportServer: 初始化server,ioMode  NIO
18/02/18 21:26:53 INFO TransportServer: Shuffle server started on port: 49559
18/02/18 21:26:53 INFO Utils: Successfully started service 'sparkDriver' on port 49559.
18/02/18 21:26:53 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:26:53 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:53 WARN SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
18/02/18 21:26:53 INFO SparkEnv: =============1.3 Sc create mapOutputTracker  ===================
 isDriver=> true

18/02/18 21:26:53 INFO SparkEnv: Registering MapOutputTracker
18/02/18 21:26:53 ERROR MapOutputTrackerMasterEndpoint: shuffler 初始化 >>  MapOutputTrackerMasterEndpoint
18/02/18 21:26:53 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:26:53 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:53 INFO SparkEnv: Registering BlockManagerMaster
18/02/18 21:26:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/02/18 21:26:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/02/18 21:26:53 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:53 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:53 INFO DiskBlockManager: Created local directory at /private/var/folders/l1/gj8n1knn3ng6z6ss4y9qs0br0000gn/T/blockmgr-8058da65-5f0c-4b6c-a725-685de055e827
18/02/18 21:26:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/02/18 21:26:53 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:26:53 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:53 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/18 21:26:53 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:26:53 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:53 ERROR SparkEnv: driver 临时目录 /private/var/folders/l1/gj8n1knn3ng6z6ss4y9qs0br0000gn/T/spark-2a7227f9-50bb-48a2-8082-502a64c3709c/userFiles-6a55194b-f11d-4211-876c-4b8c86093718
18/02/18 21:26:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/18 21:26:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.20.10.4:4040
18/02/18 21:26:54 INFO SparkContext: Added JAR file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar at spark://172.20.10.4:49559/jars/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar with timestamp 1518960414263
18/02/18 21:26:54 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:26:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:54 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver), ExpireDeadHosts)
18/02/18 21:26:54 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:26:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:54 INFO SparkContext:
 ========================= 1.4 sc schedulerBackend ===========================
 _schedulerBackend =MesosCoarseGrainedSchedulerBackend

18/02/18 21:26:54 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver), TaskSchedulerIsSet)
18/02/18 21:26:54 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:26:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:54 ERROR TaskSchedulerImpl: backend Started ==================
18/02/18 21:26:54 INFO HeartbeatReceiver: driver taskcheduler生成  => org.apache.spark.scheduler.TaskSchedulerImpl@610d073c
18/02/18 21:26:54 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStart$
18/02/18 21:26:54 INFO MesosCoarseGrainedSchedulerBackend:
########################   4 CoarseGrainedSchedulerBackend start
 driverEndpoint = 172.20.10.4:49559
 properties = (spark.driver.host,172.20.10.4),(spark.driver.memory,1G),(spark.master,mesos://localhost:5050),(spark.app.name,Spark WC),(spark.default.parallelism,8),(spark.cores.max,2),(spark.executor.memory,2G),(spark.jars,file:/Users/spafka/Desktop/flink/spark/shell/../examples/target/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar),(spark.executor.id,driver),(spark.submit.deployMode,client),(spark.driver.port,49559),(spark.logConf,true)
########################

18/02/18 21:26:54 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


WARNING: Logging before InitGoogleLogging() is written to STDERR
W0218 21:26:54.423920 59273216 sched.cpp:1727]
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
18/02/18 21:26:54 ERROR MesosCoarseGrainedSchedulerBackend: mesos driver created=> org.apache.mesos.MesosSchedulerDriver@5ffc5491
I0218 21:26:54.433688 117145600 sched.cpp:232] Version: 1.4.1
I0218 21:26:54.435072 111792128 sched.cpp:336] New master detected at master@127.0.0.1:5050
I0218 21:26:54.435286 111792128 sched.cpp:352] No credentials provided. Attempting to register without authentication
I0218 21:26:54.438519 115011584 sched.cpp:759] Framework registered with da4b5902-048b-4f7b-ab2a-00bf8afadf33-0000
18/02/18 21:26:54 INFO TransportServer: 初始化TransPortserver ...
18/02/18 21:26:54 INFO TransportServer: 初始化server,ioMode  NIO
18/02/18 21:26:54 INFO TransportServer: Shuffle server started on port: 49563
18/02/18 21:26:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49563.
18/02/18 21:26:54 INFO NettyBlockTransferService: Server created on 172.20.10.4:49563
18/02/18 21:26:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/02/18 21:26:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.20.10.4, 49563, None)
18/02/18 21:26:54 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), RegisterBlockManager(BlockManagerId(driver, 172.20.10.4, 49563, None),384093388,0,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)))
18/02/18 21:26:54 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:54 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.10.4:49563 with 366.3 MB RAM, BlockManagerId(driver, 172.20.10.4, 49563, None)
18/02/18 21:26:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.20.10.4, 49563, None)
18/02/18 21:26:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.20.10.4, 49563, None)
18/02/18 21:26:54 INFO MesosCoarseGrainedSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/02/18 21:26:54 ERROR MesosCoarseGrainedSchedulerBackend: driver submit taskeds Map(value: "da4b5902-048b-4f7b-ab2a-00bf8afadf33-O0"
 -> List(name: "Spark WC 0"
task_id {
  value: "0"
}
slave_id {
  value: "da4b5902-048b-4f7b-ab2a-00bf8afadf33-S0"
}
resources {
  name: "cpus"
  type: SCALAR
  scalar {
    value: 2.0
  }
  role: "*"
}
resources {
  name: "mem"
  type: SCALAR
  scalar {
    value: 2432.0
  }
  role: "*"
}
command {
  uris {
    value: "/Users/spafka/Desktop/flink/spark/spark-2.4.0-SNAPSHOT-bin-2.6.5.tgz"
    cache: false
  }
  environment {
    variables {
      name: "SPARK_EXECUTOR_OPTS"
      value: ""
    }
    variables {
      name: "SPARK_USER"
      value: "spafka"
    }
    variables {
      name: "SPARK_EXECUTOR_MEMORY"
      value: "2048m"
    }
  }
  value: "cd spark-2*;  ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.20.10.4:49559 --executor-id 0 --hostname localhost --cores 2 --app-id da4b5902-048b-4f7b-ab2a-00bf8afadf33-0000"
}
container {
  type: MESOS
}
labels {
}
))
18/02/18 21:26:54 ERROR MesosCoarseGrainedSchedulerBackend: Accepting offer: da4b5902-048b-4f7b-ab2a-00bf8afadf33-O0 with attributes: Map() reservation info: mem: 31744.0 cpu: 8.0 ports: List((31000,32000)) resources: name: "cpus"
type: SCALAR
scalar {
  value: 8.0
}
role: "*"
allocation_info {
  role: "*"
}
,name: "mem"
type: SCALAR
scalar {
  value: 31744.0
}
role: "*"
allocation_info {
  role: "*"
}
,name: "disk"
type: SCALAR
scalar {
  value: 131328.0
}
role: "*"
allocation_info {
  role: "*"
}
,name: "ports"
type: RANGES
ranges {
  range {
    begin: 31000
    end: 32000
  }
}
role: "*"
allocation_info {
  role: "*"
}
.  Launching 1 Mesos tasks.
18/02/18 21:26:54 ERROR MesosCoarseGrainedSchedulerBackend: Launching Mesos task: 0 with mem: 2432.0 cpu: 2.0 ports:  on slave with slave id: da4b5902-048b-4f7b-ab2a-00bf8afadf33-S0
18/02/18 21:26:55 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:55 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


18/02/18 21:26:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 366.1 MB)
18/02/18 21:26:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 366.1 MB)
18/02/18 21:26:55 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), UpdateBlockInfo(BlockManagerId(driver, 172.20.10.4, 49563, None),broadcast_0_piece0,StorageLevel(memory, 1 replicas),20888,0))
18/02/18 21:26:55 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:55 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.10.4:49563 (size: 20.4 KB, free: 366.3 MB)
18/02/18 21:26:55 INFO SparkContext: Created broadcast 0 from textFile at SparkWc.scala:42
18/02/18 21:26:55 INFO SparkContext: Starting job: collect at SparkWc.scala:52
18/02/18 21:26:55 WARN DAGScheduler: ====================2.1 spark  start a Job====================
18/02/18 21:26:55 WARN DAGScheduler: spark 提交Job rdd:ShuffledRDD[4] at combineByKey at SparkWc.scala:45，
func：<function2>,
partitions:0,1,2,3,4,5,6,7,
callSite:CallSite(collect at SparkWc.scala:52,org.apache.spark.rdd.RDD.collect(RDD.scala:939)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:52)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
resultHandler:<function2>,
properties:{spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}})
18/02/18 21:26:55 INFO DAGScheduler:
 ****************** 2.2 spark submit an jobSubmitEvent ******************
 spark  DAGSchedulerEventProcessLoop post an JobSubmitEvent
 jobId => 0,
 rdd => ShuffledRDD[4] at combineByKey at SparkWc.scala:45,
 func2 => <function2>,
 partitions.toArray =>0,1,2,3,4,5,6,7,
 callSite => CallSite(collect at SparkWc.scala:52,org.apache.spark.rdd.RDD.collect(RDD.scala:939)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:52)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
 waiter => org.apache.spark.scheduler.JobWaiter@6535117e,
 properties=>{spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}}
 ******************

18/02/18 21:26:55 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.JobSubmitted
18/02/18 21:26:55 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept JobSubmitted ******************
 jobId => 0,
 rdd => ShuffledRDD[4] at combineByKey at SparkWc.scala:45,
 func2 => <function2>,
 partitions.toArray =>0,1,2,3,4,5,6,7,
 callSite => CallSite(collect at SparkWc.scala:52,org.apache.spark.rdd.RDD.collect(RDD.scala:939)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:52)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
 listener => org.apache.spark.scheduler.JobWaiter@6535117e,
 properties=>{spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}}

18/02/18 21:26:55 INFO DAGScheduler: *********************** spark 2.3.1 createResultStage
**********************

18/02/18 21:26:55 INFO FileInputFormat: Total input paths to process : 1
18/02/18 21:26:55 INFO ShuffledRDD:
 ShuffledRDD.getDependencies
 deps= List(org.apache.spark.ShuffleDependency@5030ab3a)

18/02/18 21:26:55 INFO DAGScheduler:
********************** 2.3.3 spark createShuffleMapStage
 stage =
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set()


18/02/18 21:26:56 INFO DAGScheduler: mapOutputTracker => Registering RDD 3 (map at SparkWc.scala:44)
18/02/18 21:26:56 INFO DAGScheduler:
18/02/18 21:26:56 INFO DAGScheduler: Got job 0 (collect at SparkWc.scala:52) with 8 output partitions
18/02/18 21:26:56 INFO DAGScheduler: Final stage: ResultStage 1 (collect at SparkWc.scala:52)
18/02/18 21:26:56 INFO DAGScheduler: Parents of final stage: List(
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set()
     )
18/02/18 21:26:56 INFO DAGScheduler: Missing parents: List(
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set()
     )
18/02/18 21:26:56 INFO DAGScheduler: submitStage(ResultStage 1)
18/02/18 21:26:56 INFO DAGScheduler: submitStage(
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set()
     )
18/02/18 21:26:56 INFO DAGScheduler: Submitting =================
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set()
      (MapPartitionsRDD[3] at map at SparkWc.scala:44), which has no missing parents
18/02/18 21:26:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.4 KB, free 366.1 MB)
18/02/18 21:26:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 366.1 MB)
18/02/18 21:26:56 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), UpdateBlockInfo(BlockManagerId(driver, 172.20.10.4, 49563, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2542,0))
18/02/18 21:26:56 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:56 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.10.4:49563 (size: 2.5 KB, free: 366.3 MB)
18/02/18 21:26:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1086
18/02/18 21:26:56 INFO DAGScheduler:
 ********************** 2.3.4 spark create TaskBinary
 task = class org.apache.spark.scheduler.ShuffleMapStage
 rdd = MapPartitionsRDD[3] at map at SparkWc.scala:44
 taskBinaryBytes = [B@231ebf7f
 taskBinary = 1
 **********************

18/02/18 21:26:56 INFO DAGScheduler:
 *********************** spark 2.4.1 submit missing tasks


18/02/18 21:26:56 INFO DAGScheduler: Submitting 2 missing tasks from
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set(0, 1)
      (MapPartitionsRDD[3] at map at SparkWc.scala:44)
(first 15 tasks are for partitions Vector(0, 1))
18/02/18 21:26:56 INFO DAGScheduler: *************** spark 2.4.2
taskScheduler.submitTasks TaskSet(0.0, ShuffleMapTask(0, 0),ShuffleMapTask(0, 1), 0, 0, 0, {spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}})

18/02/18 21:26:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/02/18 21:26:56 INFO TaskSchedulerImpl: !!!!!!!!!!!!!!!!!!!!! spark 3.1 Task 唤醒
 backend =MesosCoarseGrainedSchedulerBackend

18/02/18 21:26:56 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:56 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


18/02/18 21:26:56 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:26:56 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:56 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


18/02/18 21:26:56 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:26:57 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:57 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


18/02/18 21:26:57 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:26:58 INFO MesosCoarseGrainedSchedulerBackend: Mesos task 0 is now TASK_RUNNING
18/02/18 21:26:58 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:58 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


18/02/18 21:26:58 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:26:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector()


18/02/18 21:26:59 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:26:59 ERROR MesosCoarseGrainedSchedulerBackend: driver submit taskeds Map()
18/02/18 21:26:59 INFO TransportServer: Netty  bootstrap childhandler org.apache.spark.rpc.netty.NettyRpcHandler@3fffff43
18/02/18 21:26:59 INFO TransportContext: Netty 初始化handler >> org.apache.spark.network.server.TransportChannelHandler@4dec1528
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 5336858701934895175
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:59 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 5380933772225568850
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:26:59 INFO TransportServer: Netty  bootstrap childhandler org.apache.spark.rpc.netty.NettyRpcHandler@3fffff43
18/02/18 21:26:59 INFO TransportContext: Netty 初始化handler >> org.apache.spark.network.server.TransportChannelHandler@358ab378
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:26:59 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8596626168437327589
18/02/18 21:26:59 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:26:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 6743727524273916719
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 5058703032864553733
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 6120166985902000378
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 9143488404451222323
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef( conf=null, endpointAddress=spark-client://Executor name=Executor) (172.20.10.4:49571) with ID 0
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint:
18/02/18 21:27:00 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver), ExecutorRegistered(0))
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO HeartbeatReceiver: driver ExecutorRegistered 注册 =>0
18/02/18 21:27:00 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.ExecutorAdded
18/02/18 21:27:00 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:27:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor 0, partition 0, PROCESS_LOCAL, 7912 bytes)
18/02/18 21:27:00 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:00 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ShuffleMapTask(0, 0)
taskInfo =org.apache.spark.scheduler.TaskInfo@726ef261
******************
18/02/18 21:27:00 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor 0, partition 1, PROCESS_LOCAL, 7912 bytes)
18/02/18 21:27:00 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:00 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ShuffleMapTask(0, 1)
taskInfo =org.apache.spark.scheduler.TaskInfo@49389cc9
******************
TaskDescription(taskId=0,
 attemptNumber=0,
 executorId=0,
 name=task 0.0 in stage 0.0, index=0,
 addedFiles=Map(),
 addedJars=Map(spark://172.20.10.4:49559/jars/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar -> 1518960414263),
 properties={spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}},
 serializedTask=-84-19051151140411111141034697112979910410146115112971141074611599104101100117108101114468310411710210210810177971128497115107-60-42-36041-104100982027609112971141161051161051111101160287611111410347971129799104101471151129711410747809711411610511610511111059760101169711510766105110971141211160387611111410347971129799104101471151129711410747981141119710099971151164766114111971009997115116591201140311111141034697112979910410146115112971141074611599104101100117108101114468497115107-9074-20198510353720117402795101120101991171161111146810111510111410597108105122101671121178410510910174024951011201019911711611111468101115101114105971081051221018410510910174051011121119910473011112971141161051161051111107310073014115116971031016511611610110911211673100730711511697103101731007601297112112651161161011091121167310011601476115999710897477911211610511111059760597112112731001130126047605106111987310011301260491021115101114105971081051221011008497115107771011161141059911511602916676017116971151077710110911111412177971109710310111411604376111114103479711297991041014711511297114107471091011091111141214784971151077710110911111412177971109710310111459120112000000000000000000000000000000000000115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-371410211620012011211511401011599971089746831111091011734-1410594-95-1171162017601120116018761069711897471089711010347799810610199116591201130126091160411009752985357485045485256984552102559845979850974548489810256971029710010251514548484848115113012601111511401710697118974610897110103467311011610110310111418-30-96-92-9-127-1215620173051189710811710112011401610697118974610897110103467811710998101114-122-84-1072911-108-32-1172001201120000117114029166-84-1323-86884-322001201120023-88-84-1905115114037111114103469711297991041014611511297114107461011201019911711611111446849711510777101116114105991157-2285-112-72-481251072016760179510010511510766121116101115831121051081081011001160397611111410347971129799104101471151129711410747117116105108477611111010365999911710911710897116111114597601695101120101991171161111146711211784105109101113012601760279510112010199117116111114681011151011141059710810512210167112117841051091011130126017602495101120101991171161111146810111510111410597108105122101841051091011130126017601695101120101991171161111148211711084105109101113012601760109510611810971678410510910111301260176019951091011091111141216612111610111583112105108108101100113012601760209511210197107691201019911711610511111077101109111114121113012601760249511410111511710811683101114105971081051229711610511111084105109101113012601760219511711210097116101100661081119910783116971161171151011151160457611111410347971129799104101471151129711410747117116105108476711110810810199116105111110659999117109117108971161111145976012105110112117116771011161141059911511604076111114103479711297991041014711511297114107471011201019911711611111447731101121171167710111611410599115597605011111410336971129799104101361151129711410736101120101991171161111143684971151077710111611410599115363695114101115117108116831051221011130126017601311111711611211711677101116114105991151160417611111410347971129799104101471151129711410747101120101991171161111144779117116112117116771011161141059911559760181151041171021021081018210197100771011161141059911511604676111114103479711297991041014711511297114107471011201019911711611111447831041171021021081018210197100771011161141059911559760191151041171021021081018711410511610177101116114105991151160477611111410347971129799104101471151129711410747101120101991171161111144783104117102102108101871141051161017710111611410599115597609116101115116659999117109116014761159997108974779112116105111110591201121151140371111141034697112979910410146115112971141074611711610510846761111101036599991171091171089711611111453127-33-2-1145987572027406959911111711011674049511511710912011403511111410346971129799104101461151129711410746117116105108466599991171091171089711611111486507329-1542-7014100252029004911111410336971129799104101361151129711410736117116105108366599991171091171089711611111486503636971166811410511810111483105100101760810910111697100971169711604376111114103479711297991041014711511297114107471171161051084765999911710911710897116111114771011169710097116975912011211151140411111141034697112979910410146115112971141074611711610510846659999117109117108971161111147710111697100971169712226-73-20117123-4-862039001799111117110116709710510810110086971081171011157402105100760411097109101113012607120112100000008115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-37141021162001201120000000000000000115113012609111511301260131000000031130126017000000000000000011511301260911151130126013100000001113012601700000000000000001151130126091115113012601310000000011301260170000000000000000115113012609111511301260131000000021130126017000000000000000011511301260911151130126013100000005113012601700000000000000001151130126091115113012601310000000711301260170000000000000000115113012609111511301260131000000091130126017000000000000000011511301260911151130126013100000006113012601700000000000000001151140431111141034697112979910410146115112971141074611711610510846671111081081019911610511111065999911710911710897116111114-1977-69-8372-93-3919201760595108105115116116016761069711897471171161051084776105115116591201130126010111511301260131000000010113012601711511403810697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110076105115116-10899-17-29-12568161242017604108105115116113012603512011404410697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110067111108108101991161051111104297-8779-100-103-75302760199116022761069711897471171161051084767111108108101991161051111105976051091171161011201160187610697118974710897110103477998106101991165912011211511401910697118974611711610510846651141149712176105115116120-127-4629-103-5797-99301730411510512210112011200001194000012011301260421201130126044115114038111114103469711297991041014611511297114107461011201019911711611111446731101121171167710111611410599115-380118108115-21-66682027601095981211161011158210197100113012601760129511410199111114100115821019710011301260112011211511301260911151130126013100000002111301260170000000000000000115113012609111511301260131000000022113012601700000000000000001151130126091115113012601310000000411301260170000000000000000115114039111114103469711297991041014611511297114107461011201019911711611111446791171161121171167710111611410599115585960-31-552-7-3320276013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126011201121151130126091115113012601310000000231130126017000000000000000011511301260911151130126013100000002411301260170000000000000000115114044111114103469711297991041014611511297114107461011201019911711611111446831041171021021081018210197100771011161141059911553-13-1697794-119-1620776014951021011169910487971051168410510910111301260176019951081119997108661081119910711570101116991041011001130126017601595108111999710866121116101115821019710011301260176012951141019911111410011582101971001130126017602095114101109111116101661081119910711570101116991041011001130126017601695114101109111116101661211161011158210197100113012601760229511410110911111610166121116101115821019710084111681051151071130126011201121151130126091115113012601310000000161130126017000000000000000011511301260911151130126013100000001211301260170000000000000000115113012609111511301260131000000015113012601700000000000000001151130126091115113012601310000000171130126017000000000000000011511301260911151130126013100000001111301260170000000000000000115113012609111511301260131000000013113012601700000000000000001151130126091115113012601310000000141130126017000000000000000011511404511111410346971129799104101461151129711410746101120101991171161111144683104117102102108101871141051161017710111611410599115-2845112-89-29281269620376013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126017601095119114105116101841051091011130126011201121151130126091115113012601310000000181130126017000000000000000011511301260911151130126013100000001911301260170000000000000000115113012609111511301260131000000020113012601700000000000000001130126017000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000112115114036111114103469711297991041014611511297114107461141001004672971001111111128097114116105116105111110-61-113-12109-78-6358822037305105110100101120730511410010073100760101051101121171168311210810511611603976111114103479711297991041014711511297114107478310111410597108105122979810810187114105116979810810159120112000000001151140371111141034697112979910410146115112971141074683101114105971081051229798108101871141051169798108101-109-12-127-1163491-76-20300120112119-113034111114103469711297991041014610497100111111112461099711211410110046701051081018311210810511603411111410346971129799104101461049710011111111246109971121141011004670105108101831121081051165410210510810158478511510111411547115112971021079747681011151071161111124710210810511010747115112971141074767797884827366858473787146109100000000000000001-151201151140431111141034697112979910410146115112971141074698114111971009997115116468411111411410111011666114111971009997115116-570-6084-51-88-395530573091101171096610811199107115900601111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363699104101991071151171096911097981081011007605611111410336971129799104101361151129711410736981141119710099971151163684111114114101110116661141119710099971151163636981141119710099971151167310011604376111114103479711297991041014711511297114107471151161111149710310147661141119710099971151166610811199107731005991054111114103369711297991041013611511297114107369811411197100999711511636841111141141011101166611411197100999711511636369910410199107115117109115116029173760551111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363610111810510010111099101364911602476115999710897471141011021081019911647671089711511584971035912011403611111410346971129799104101461151129711410746981141119710099971151164666114111971009997115116-1546-225-108244049203900895105115869710810510074021051007605011111410336971129799104101361151129711410736981141119710099971151163666114111971009997115116363695100101115116114111121831051161011160187610697118974710897110103478311611410511010359120112100000001116000001011511404111111410346971129799104101461151129711410746115116111114971031014666114111971009997115116661081119910773100-3392680-7241-38-722027401198114111971009997115116731007605102105101108100113012603112011200000001113012603311711402917377-709638118-22-78-9120012011200016-121-70110115114030115999710897461141011021081019911646671089711511584971033636971101111103649-69120-13-21-1030100-122201760151141171101161051091016710897115115493649116017761069711897471089711010347671089711511559120112118113012601912000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
TaskDescription(taskId=1,
 attemptNumber=0,
 executorId=0,
 name=task 1.0 in stage 0.0, index=1,
 addedFiles=Map(),
 addedJars=Map(spark://172.20.10.4:49559/jars/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar -> 1518960414263),
 properties={spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}},
 serializedTask=-84-19051151140411111141034697112979910410146115112971141074611599104101100117108101114468310411710210210810177971128497115107-60-42-36041-104100982027609112971141161051161051111101160287611111410347971129799104101471151129711410747809711411610511610511111059760101169711510766105110971141211160387611111410347971129799104101471151129711410747981141119710099971151164766114111971009997115116591201140311111141034697112979910410146115112971141074611599104101100117108101114468497115107-9074-20198510353720117402795101120101991171161111146810111510111410597108105122101671121178410510910174024951011201019911711611111468101115101114105971081051221018410510910174051011121119910473011112971141161051161051111107310073014115116971031016511611610110911211673100730711511697103101731007601297112112651161161011091121167310011601476115999710897477911211610511111059760597112112731001130126047605106111987310011301260491021115101114105971081051221011008497115107771011161141059911511602916676017116971151077710110911111412177971109710310111411604376111114103479711297991041014711511297114107471091011091111141214784971151077710110911111412177971109710310111459120112000000000000000000000000000100000000115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-371410211620012011211511401011599971089746831111091011734-1410594-95-1171162017601120116018761069711897471089711010347799810610199116591201130126091160411009752985357485045485256984552102559845979850974548489810256971029710010251514548484848115113012601111511401710697118974610897110103467311011610110310111418-30-96-92-9-127-1215620173051189710811710112011401610697118974610897110103467811710998101114-122-84-1072911-108-32-1172001201120000117114029166-84-1323-86884-322001201120023-88-84-1905115114037111114103469711297991041014611511297114107461011201019911711611111446849711510777101116114105991157-2285-112-72-481251072016760179510010511510766121116101115831121051081081011001160397611111410347971129799104101471151129711410747117116105108477611111010365999911710911710897116111114597601695101120101991171161111146711211784105109101113012601760279510112010199117116111114681011151011141059710810512210167112117841051091011130126017602495101120101991171161111146810111510111410597108105122101841051091011130126017601695101120101991171161111148211711084105109101113012601760109510611810971678410510910111301260176019951091011091111141216612111610111583112105108108101100113012601760209511210197107691201019911711610511111077101109111114121113012601760249511410111511710811683101114105971081051229711610511111084105109101113012601760219511711210097116101100661081119910783116971161171151011151160457611111410347971129799104101471151129711410747117116105108476711110810810199116105111110659999117109117108971161111145976012105110112117116771011161141059911511604076111114103479711297991041014711511297114107471011201019911711611111447731101121171167710111611410599115597605011111410336971129799104101361151129711410736101120101991171161111143684971151077710111611410599115363695114101115117108116831051221011130126017601311111711611211711677101116114105991151160417611111410347971129799104101471151129711410747101120101991171161111144779117116112117116771011161141059911559760181151041171021021081018210197100771011161141059911511604676111114103479711297991041014711511297114107471011201019911711611111447831041171021021081018210197100771011161141059911559760191151041171021021081018711410511610177101116114105991151160477611111410347971129799104101471151129711410747101120101991171161111144783104117102102108101871141051161017710111611410599115597609116101115116659999117109116014761159997108974779112116105111110591201121151140371111141034697112979910410146115112971141074611711610510846761111101036599991171091171089711611111453127-33-2-1145987572027406959911111711011674049511511710912011403511111410346971129799104101461151129711410746117116105108466599991171091171089711611111486507329-1542-7014100252029004911111410336971129799104101361151129711410736117116105108366599991171091171089711611111486503636971166811410511810111483105100101760810910111697100971169711604376111114103479711297991041014711511297114107471171161051084765999911710911710897116111114771011169710097116975912011211151140411111141034697112979910410146115112971141074611711610510846659999117109117108971161111147710111697100971169712226-73-20117123-4-862039001799111117110116709710510810110086971081171011157402105100760411097109101113012607120112100000008115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-37141021162001201120000000000000000115113012609111511301260131000000031130126017000000000000000011511301260911151130126013100000001113012601700000000000000001151130126091115113012601310000000011301260170000000000000000115113012609111511301260131000000021130126017000000000000000011511301260911151130126013100000005113012601700000000000000001151130126091115113012601310000000711301260170000000000000000115113012609111511301260131000000091130126017000000000000000011511301260911151130126013100000006113012601700000000000000001151140431111141034697112979910410146115112971141074611711610510846671111081081019911610511111065999911710911710897116111114-1977-69-8372-93-3919201760595108105115116116016761069711897471171161051084776105115116591201130126010111511301260131000000010113012601711511403810697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110076105115116-10899-17-29-12568161242017604108105115116113012603512011404410697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110067111108108101991161051111104297-8779-100-103-75302760199116022761069711897471171161051084767111108108101991161051111105976051091171161011201160187610697118974710897110103477998106101991165912011211511401910697118974611711610510846651141149712176105115116120-127-4629-103-5797-99301730411510512210112011200001194000012011301260421201130126044115114038111114103469711297991041014611511297114107461011201019911711611111446731101121171167710111611410599115-380118108115-21-66682027601095981211161011158210197100113012601760129511410199111114100115821019710011301260112011211511301260911151130126013100000002111301260170000000000000000115113012609111511301260131000000022113012601700000000000000001151130126091115113012601310000000411301260170000000000000000115114039111114103469711297991041014611511297114107461011201019911711611111446791171161121171167710111611410599115585960-31-552-7-3320276013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126011201121151130126091115113012601310000000231130126017000000000000000011511301260911151130126013100000002411301260170000000000000000115114044111114103469711297991041014611511297114107461011201019911711611111446831041171021021081018210197100771011161141059911553-13-1697794-119-1620776014951021011169910487971051168410510910111301260176019951081119997108661081119910711570101116991041011001130126017601595108111999710866121116101115821019710011301260176012951141019911111410011582101971001130126017602095114101109111116101661081119910711570101116991041011001130126017601695114101109111116101661211161011158210197100113012601760229511410110911111610166121116101115821019710084111681051151071130126011201121151130126091115113012601310000000161130126017000000000000000011511301260911151130126013100000001211301260170000000000000000115113012609111511301260131000000015113012601700000000000000001151130126091115113012601310000000171130126017000000000000000011511301260911151130126013100000001111301260170000000000000000115113012609111511301260131000000013113012601700000000000000001151130126091115113012601310000000141130126017000000000000000011511404511111410346971129799104101461151129711410746101120101991171161111144683104117102102108101871141051161017710111611410599115-2845112-89-29281269620376013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126017601095119114105116101841051091011130126011201121151130126091115113012601310000000181130126017000000000000000011511301260911151130126013100000001911301260170000000000000000115113012609111511301260131000000020113012601700000000000000001130126017000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000112115114036111114103469711297991041014611511297114107461141001004672971001111111128097114116105116105111110-61-113-12109-78-6358822037305105110100101120730511410010073100760101051101121171168311210810511611603976111114103479711297991041014711511297114107478310111410597108105122979810810187114105116979810810159120112000100001151140371111141034697112979910410146115112971141074683101114105971081051229798108101871141051169798108101-109-12-127-1163491-76-20300120112119-1130341111141034697112979910410146104971001111111124610997112114101100467010510810183112108105116034111114103469711297991041014610497100111111112461099711211410110046701051081018311210810511654102105108101584785115101114115471151129710210797476810111510711611111247102108105110107471151129711410747677978848273668584737871461091000000001-150000001-141201151140431111141034697112979910410146115112971141074698114111971009997115116468411111411410111011666114111971009997115116-570-6084-51-88-395530573091101171096610811199107115900601111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363699104101991071151171096911097981081011007605611111410336971129799104101361151129711410736981141119710099971151163684111114114101110116661141119710099971151163636981141119710099971151167310011604376111114103479711297991041014711511297114107471151161111149710310147661141119710099971151166610811199107731005991054111114103369711297991041013611511297114107369811411197100999711511636841111141141011101166611411197100999711511636369910410199107115117109115116029173760551111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363610111810510010111099101364911602476115999710897471141011021081019911647671089711511584971035912011403611111410346971129799104101461151129711410746981141119710099971151164666114111971009997115116-1546-225-108244049203900895105115869710810510074021051007605011111410336971129799104101361151129711410736981141119710099971151163666114111971009997115116363695100101115116114111121831051161011160187610697118974710897110103478311611410511010359120112100000001116000001011511404111111410346971129799104101461151129711410746115116111114971031014666114111971009997115116661081119910773100-3392680-7241-38-722027401198114111971009997115116731007605102105101108100113012603112011200000001113012603311711402917377-709638118-22-78-9120012011200016-121-70110115114030115999710897461141011021081019911646671089711511584971033636971101111103649-69120-13-21-1030100-122201760151141171101161051091016710897115115493649116017761069711897471089711010347671089711511559120112118113012601912000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 0 on executor id: 0
hostname:localhost
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 1 on executor id: 0
hostname:localhost
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 7017057282053554073
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO BlockManagerMasterEndpoint: Registering block manager localhost:49572 with 366.3 MB RAM, BlockManagerId(0, localhost, 49572, None)
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 4927708582972175203
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,0))


18/02/18 21:27:00 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 2
18/02/18 21:27:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:00 INFO TransportServer: Netty  bootstrap childhandler org.apache.spark.rpc.netty.NettyRpcHandler@3fffff43
18/02/18 21:27:00 INFO TransportContext: Netty 初始化handler >> org.apache.spark.network.server.TransportChannelHandler@3fb399b0
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessConnected
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8592961395775895897
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO TransportServer: Netty  bootstrap childhandler org.apache.spark.network.netty.NettyBlockRpcServer@273cc580
18/02/18 21:27:00 INFO TransportContext: Netty 初始化handler >> org.apache.spark.network.server.TransportChannelHandler@623e5f03
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 7720627751714877966
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 6306167507381555018
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:49572 (size: 2.5 KB, free: 366.3 MB)
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 6663821480799446794
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8434504073301727388
18/02/18 21:27:00 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 6243193808347192799
18/02/18 21:27:00 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:49572 (size: 20.4 KB, free: 366.3 MB)
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/18 21:27:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 783 ms on localhost (executor 0) (1/2)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 767 ms on localhost (executor 0) (2/2)
18/02/18 21:27:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ShuffleMapTask(0, 0),Success,org.apache.spark.scheduler.CompressedMapStatus@d245f5,ArrayBuffer(LongAccumulator(id: 0, name: Some(internal.metrics.executorDeserializeTime), value: 418), LongAccumulator(id: 1, name: Some(internal.metrics.executorDeserializeCpuTime), value: 152083000), LongAccumulator(id: 2, name: Some(internal.metrics.executorRunTime), value: 229), LongAccumulator(id: 3, name: Some(internal.metrics.executorCpuTime), value: 152821000), LongAccumulator(id: 4, name: Some(internal.metrics.resultSize), value: 1195), LongAccumulator(id: 5, name: Some(internal.metrics.jvmGCTime), value: 15), LongAccumulator(id: 6, name: Some(internal.metrics.resultSerializationTime), value: 1), LongAccumulator(id: 7, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 8, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 9, name: Some(internal.metrics.peakExecutionMemory), value: 6192), LongAccumulator(id: 18, name: Some(internal.metrics.shuffle.write.bytesWritten), value: 1696), LongAccumulator(id: 19, name: Some(internal.metrics.shuffle.write.recordsWritten), value: 57), LongAccumulator(id: 20, name: Some(internal.metrics.shuffle.write.writeTime), value: 2860258), LongAccumulator(id: 21, name: Some(internal.metrics.input.bytesRead), value: 995), LongAccumulator(id: 22, name: Some(internal.metrics.input.recordsRead), value: 9)),org.apache.spark.scheduler.TaskInfo@726ef261) ******************
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ShuffleMapTask(0, 1),Success,org.apache.spark.scheduler.CompressedMapStatus@2cc436d5,ArrayBuffer(LongAccumulator(id: 0, name: Some(internal.metrics.executorDeserializeTime), value: 418), LongAccumulator(id: 1, name: Some(internal.metrics.executorDeserializeCpuTime), value: 191841000), LongAccumulator(id: 2, name: Some(internal.metrics.executorRunTime), value: 229), LongAccumulator(id: 3, name: Some(internal.metrics.executorCpuTime), value: 121213000), LongAccumulator(id: 4, name: Some(internal.metrics.resultSize), value: 1195), LongAccumulator(id: 5, name: Some(internal.metrics.jvmGCTime), value: 15), LongAccumulator(id: 6, name: Some(internal.metrics.resultSerializationTime), value: 1), LongAccumulator(id: 7, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 8, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 9, name: Some(internal.metrics.peakExecutionMemory), value: 5749), LongAccumulator(id: 18, name: Some(internal.metrics.shuffle.write.bytesWritten), value: 1592), LongAccumulator(id: 19, name: Some(internal.metrics.shuffle.write.recordsWritten), value: 54), LongAccumulator(id: 20, name: Some(internal.metrics.shuffle.write.writeTime), value: 2853055), LongAccumulator(id: 21, name: Some(internal.metrics.input.bytesRead), value: 498), LongAccumulator(id: 22, name: Some(internal.metrics.input.recordsRead), value: 7)),org.apache.spark.scheduler.TaskInfo@49389cc9) ******************
18/02/18 21:27:01 INFO DAGScheduler:
id:0
rdd: MapPartitionsRDD[3] at map at SparkWc.scala:44
numTasks: 2,
parents: List(),
firstJobId: 0,
callSite: CallSite(map at SparkWc.scala:44,org.apache.spark.rdd.RDD.map(RDD.scala:371)
org.apache.spark.examples.SparkWc$.main(SparkWc.scala:44)
org.apache.spark.examples.SparkWc.main(SparkWc.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:884)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:232)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)),
shuffleDep: 0
mapOutputTrackerMaster:172.20.10.4:49559)
_mapStageJobs: List()
pendingPartitions: Set()
      (map at SparkWc.scala:44) finished in 4.999 s
18/02/18 21:27:01 INFO DAGScheduler: looking for newly runnable stages
18/02/18 21:27:01 INFO DAGScheduler: running: Set()
18/02/18 21:27:01 INFO DAGScheduler: waiting: Set(ResultStage 1)
18/02/18 21:27:01 INFO DAGScheduler: failed: Set()
18/02/18 21:27:01 INFO DAGScheduler: submitStage(ResultStage 1)
18/02/18 21:27:01 INFO DAGScheduler: Submitting ================= ResultStage 1 (ShuffledRDD[4] at combineByKey at SparkWc.scala:45), which has no missing parents
18/02/18 21:27:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.8 KB, free 366.1 MB)
18/02/18 21:27:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1637.0 B, free 366.1 MB)
18/02/18 21:27:01 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), UpdateBlockInfo(BlockManagerId(driver, 172.20.10.4, 49563, None),broadcast_2_piece0,StorageLevel(memory, 1 replicas),1637,0))
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.20.10.4:49563 (size: 1637.0 B, free: 366.3 MB)
18/02/18 21:27:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1086
18/02/18 21:27:01 INFO DAGScheduler:
 ********************** 2.3.4 spark create TaskBinary
 task = class org.apache.spark.scheduler.ResultStage
 rdd = ShuffledRDD[4] at combineByKey at SparkWc.scala:45
 taskBinaryBytes = [B@16d5cf27
 taskBinary = 2
 **********************

18/02/18 21:27:01 INFO DAGScheduler:
 *********************** spark 2.4.1 submit missing tasks


18/02/18 21:27:01 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (ShuffledRDD[4] at combineByKey at SparkWc.scala:45)
(first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
18/02/18 21:27:01 INFO DAGScheduler: *************** spark 2.4.2
taskScheduler.submitTasks TaskSet(1.0, ResultTask(1, 0),ResultTask(1, 1),ResultTask(1, 2),ResultTask(1, 3),ResultTask(1, 4),ResultTask(1, 5),ResultTask(1, 6),ResultTask(1, 7), 1, 0, 0, {spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}})

18/02/18 21:27:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks
18/02/18 21:27:01 INFO TaskSchedulerImpl: !!!!!!!!!!!!!!!!!!!!! spark 3.1 Task 唤醒
 backend =MesosCoarseGrainedSchedulerBackend

18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
TaskDescription(taskId=2,
 attemptNumber=0,
 executorId=0,
 name=task 0.0 in stage 1.0, index=0,
 addedFiles=Map(),
 addedJars=Map(spark://172.20.10.4:49559/jars/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar -> 1518960414263),
 properties={spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}},
 serializedTask=-84-1905115114037111114103469711297991041014611511297114107461159910410110011710810111446821011151171081168497115107-123-8238123-9838-47-10920473081111171161121171167310076031081111031160187611111410347115108102521064776111103103101114597609112971141161051161051111101160287611111410347971129799104101471151129711410747809711411610511610511111059760101169711510766105110971141211160387611111410347971129799104101471151129711410747981141119710099971151164766114111971009997115116591201140311111141034697112979910410146115112971141074611599104101100117108101114468497115107-9074-20198510353720117402795101120101991171161111146810111510111410597108105122101671121178410510910174024951011201019911711611111468101115101114105971081051221018410510910174051011121119910473011112971141161051161051111107310073014115116971031016511611610110911211673100730711511697103101731007601297112112651161161011091121167310011601476115999710897477911211610511111059760597112112731001130126057605106111987310011301260591021115101114105971081051221011008497115107771011161141059911511602916676017116971151077710110911111412177971109710310111411604376111114103479711297991041014711511297114107471091011091111141214784971151077710110911111412177971109710310111459120112000000000000000000000001000000000001115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-371410211620012011211511401011599971089746831111091011734-1410594-95-11711620176011201160187610697118974710897110103477998106101991165912011301260101160411009752985357485045485256984552102559845979850974548489810256971029710010251514548484848115113012601211511401710697118974610897110103467311011610110310111418-30-96-92-9-127-1215620173051189710811710112011401610697118974610897110103467811710998101114-122-84-1072911-108-32-1172001201120000117114029166-84-1323-86884-322001201120023-88-84-1905115114037111114103469711297991041014611511297114107461011201019911711611111446849711510777101116114105991157-2285-112-72-481251072016760179510010511510766121116101115831121051081081011001160397611111410347971129799104101471151129711410747117116105108477611111010365999911710911710897116111114597601695101120101991171161111146711211784105109101113012601760279510112010199117116111114681011151011141059710810512210167112117841051091011130126017602495101120101991171161111146810111510111410597108105122101841051091011130126017601695101120101991171161111148211711084105109101113012601760109510611810971678410510910111301260176019951091011091111141216612111610111583112105108108101100113012601760209511210197107691201019911711610511111077101109111114121113012601760249511410111511710811683101114105971081051229711610511111084105109101113012601760219511711210097116101100661081119910783116971161171151011151160457611111410347971129799104101471151129711410747117116105108476711110810810199116105111110659999117109117108971161111145976012105110112117116771011161141059911511604076111114103479711297991041014711511297114107471011201019911711611111447731101121171167710111611410599115597605011111410336971129799104101361151129711410736101120101991171161111143684971151077710111611410599115363695114101115117108116831051221011130126017601311111711611211711677101116114105991151160417611111410347971129799104101471151129711410747101120101991171161111144779117116112117116771011161141059911559760181151041171021021081018210197100771011161141059911511604676111114103479711297991041014711511297114107471011201019911711611111447831041171021021081018210197100771011161141059911559760191151041171021021081018711410511610177101116114105991151160477611111410347971129799104101471151129711410747101120101991171161111144783104117102102108101871141051161017710111611410599115597609116101115116659999117109116014761159997108974779112116105111110591201121151140371111141034697112979910410146115112971141074611711610510846761111101036599991171091171089711611111453127-33-2-1145987572027406959911111711011674049511511710912011403511111410346971129799104101461151129711410746117116105108466599991171091171089711611111486507329-1542-7014100252029004911111410336971129799104101361151129711410736117116105108366599991171091171089711611111486503636971166811410511810111483105100101760810910111697100971169711604376111114103479711297991041014711511297114107471171161051084765999911710911710897116111114771011169710097116975912011211151140411111141034697112979910410146115112971141074611711610510846659999117109117108971161111147710111697100971169712226-73-20117123-4-8620390017991111171101167097105108101100869710811710111574021051007604110971091011130126071201121000000033115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-3714102116200120112000000000000000011511301260911151130126013100000002811301260170000000000000000115113012609111511301260131000000026113012601700000000000000001151130126091115113012601310000000251130126017000000000000000011511301260911151130126013100000002711301260170000000000000000115113012609111511301260131000000030113012601700000000000000001151130126091115113012601310000000321130126017000000000000000011511301260911151130126013100000003411301260170000000000000000115113012609111511301260131000000031113012601700000000000000001151140431111141034697112979910410146115112971141074611711610510846671111081081019911610511111065999911710911710897116111114-1977-69-8372-93-3919201760595108105115116116016761069711897471171161051084776105115116591201130126010111511301260131000000035113012601711511403810697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110076105115116-10899-17-29-12568161242017604108105115116113012603512011404410697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110067111108108101991161051111104297-8779-100-103-75302760199116022761069711897471171161051084767111108108101991161051111105976051091171161011201160187610697118974710897110103477998106101991165912011211511401910697118974611711610510846651141149712176105115116120-127-4629-103-5797-99301730411510512210112011200001194000012011301260421201130126044115114038111114103469711297991041014611511297114107461011201019911711611111446731101121171167710111611410599115-380118108115-21-666820276010959812111610111582101971001130126017601295114101991111141001158210197100113012601120112115113012609111511301260131000000046113012601700000000000000001151130126091115113012601310000000471130126017000000000000000011511301260911151130126013100000002911301260170000000000000000115114039111114103469711297991041014611511297114107461011201019911711611111446791171161121171167710111611410599115585960-31-552-7-3320276013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126011201121151130126091115113012601310000000481130126017000000000000000011511301260911151130126013100000004911301260170000000000000000115114044111114103469711297991041014611511297114107461011201019911711611111446831041171021021081018210197100771011161141059911553-13-1697794-119-1620776014951021011169910487971051168410510910111301260176019951081119997108661081119910711570101116991041011001130126017601595108111999710866121116101115821019710011301260176012951141019911111410011582101971001130126017602095114101109111116101661081119910711570101116991041011001130126017601695114101109111116101661211161011158210197100113012601760229511410110911111610166121116101115821019710084111681051151071130126011201121151130126091115113012601310000000411130126017000000000000000011511301260911151130126013100000003711301260170000000000000000115113012609111511301260131000000040113012601700000000000000001151130126091115113012601310000000421130126017000000000000000011511301260911151130126013100000003611301260170000000000000000115113012609111511301260131000000038113012601700000000000000001151130126091115113012601310000000391130126017000000000000000011511404511111410346971129799104101461151129711410746101120101991171161111144683104117102102108101871141051161017710111611410599115-2845112-89-29281269620376013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126017601095119114105116101841051091011130126011201121151130126091115113012601310000000431130126017000000000000000011511301260911151130126013100000004411301260170000000000000000115113012609111511301260131000000045113012601700000000000000001130126017000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000112000011511403311111410346115108102521064610510911210846761111035210676111103103101114651009711211610111485-51-4154-67-29-11-4720190012116114979910167971129798108101120114036111114103461151081025210646104101108112101114115467797114107101114731031101111141051101036697115101125-125-7985789339-101200120114033111114103461151081025210646104101108112101114115467897109101100761111031031011146697115101104-110-99-5628788512520176041109710910111601876106971189747108971101034783116114105110103591201121160108210111511710811684971151071115114041111114103469711297991041014611511297114107461141001004683104117102102108101100826868809711411610511610511111098-109119-125-85-11348-1020273031051001207305105110100101120120112000000001151140431111141034697112979910410146115112971141074698114111971009997115116468411111411410111011666114111971009997115116-570-6084-51-88-395530573091101171096610811199107115900601111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363699104101991071151171096911097981081011007605611111410336971129799104101361151129711410736981141119710099971151163684111114114101110116661141119710099971151163636981141119710099971151167310011604376111114103479711297991041014711511297114107471151161111149710310147661141119710099971151166610811199107731005991054111114103369711297991041013611511297114107369811411197100999711511636841111141141011101166611411197100999711511636369910410199107115117109115116029173760551111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363610111810510010111099101364911602476115999710897471141011021081019911647671089711511584971035912011403611111410346971129799104101461151129711410746981141119710099971151164666114111971009997115116-1546-225-108244049203900895105115869710810510074021051007605011111410336971129799104101361151129711410736981141119710099971151163666114111971009997115116363695100101115116114111121831051161011130126025120112100000002116000001011511404111111410346971129799104101461151129711410746115116111114971031014666114111971009997115116661081119910773100-3392680-7241-38-722027401198114111971009997115116731007605102105101108100113012602512011200000002113012603611711402917377-709638118-22-78-9120012011200013843-57-106115114030115999710897461141011021081019911646671089711511584971033636971101111103649-69120-13-21-1030100-1222017601511411711011610510910167108971151154936491160177610697118974710897110103476710897115115591201121181130126020120000000000000000000000000000000000000000000000000)
TaskDescription(taskId=3,
 attemptNumber=0,
 executorId=0,
 name=task 1.0 in stage 1.0, index=1,
 addedFiles=Map(),
 addedJars=Map(spark://172.20.10.4:49559/jars/original-spark-examples_2.11-2.4.0-SNAPSHOT.jar -> 1518960414263),
 properties={spark.rdd.scope.noOverride=true, spark.rdd.scope={"id":"4","name":"collect"}},
 serializedTask=-84-1905115114037111114103469711297991041014611511297114107461159910410110011710810111446821011151171081168497115107-123-8238123-9838-47-10920473081111171161121171167310076031081111031160187611111410347115108102521064776111103103101114597609112971141161051161051111101160287611111410347971129799104101471151129711410747809711411610511610511111059760101169711510766105110971141211160387611111410347971129799104101471151129711410747981141119710099971151164766114111971009997115116591201140311111141034697112979910410146115112971141074611599104101100117108101114468497115107-9074-20198510353720117402795101120101991171161111146810111510111410597108105122101671121178410510910174024951011201019911711611111468101115101114105971081051221018410510910174051011121119910473011112971141161051161051111107310073014115116971031016511611610110911211673100730711511697103101731007601297112112651161161011091121167310011601476115999710897477911211610511111059760597112112731001130126057605106111987310011301260591021115101114105971081051221011008497115107771011161141059911511602916676017116971151077710110911111412177971109710310111411604376111114103479711297991041014711511297114107471091011091111141214784971151077710110911111412177971109710310111459120112000000000000000000000001000100000001115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-371410211620012011211511401011599971089746831111091011734-1410594-95-11711620176011201160187610697118974710897110103477998106101991165912011301260101160411009752985357485045485256984552102559845979850974548489810256971029710010251514548484848115113012601211511401710697118974610897110103467311011610110310111418-30-96-92-9-127-1215620173051189710811710112011401610697118974610897110103467811710998101114-122-84-1072911-108-32-1172001201120000117114029166-84-1323-86884-322001201120023-88-84-1905115114037111114103469711297991041014611511297114107461011201019911711611111446849711510777101116114105991157-2285-112-72-481251072016760179510010511510766121116101115831121051081081011001160397611111410347971129799104101471151129711410747117116105108477611111010365999911710911710897116111114597601695101120101991171161111146711211784105109101113012601760279510112010199117116111114681011151011141059710810512210167112117841051091011130126017602495101120101991171161111146810111510111410597108105122101841051091011130126017601695101120101991171161111148211711084105109101113012601760109510611810971678410510910111301260176019951091011091111141216612111610111583112105108108101100113012601760209511210197107691201019911711610511111077101109111114121113012601760249511410111511710811683101114105971081051229711610511111084105109101113012601760219511711210097116101100661081119910783116971161171151011151160457611111410347971129799104101471151129711410747117116105108476711110810810199116105111110659999117109117108971161111145976012105110112117116771011161141059911511604076111114103479711297991041014711511297114107471011201019911711611111447731101121171167710111611410599115597605011111410336971129799104101361151129711410736101120101991171161111143684971151077710111611410599115363695114101115117108116831051221011130126017601311111711611211711677101116114105991151160417611111410347971129799104101471151129711410747101120101991171161111144779117116112117116771011161141059911559760181151041171021021081018210197100771011161141059911511604676111114103479711297991041014711511297114107471011201019911711611111447831041171021021081018210197100771011161141059911559760191151041171021021081018711410511610177101116114105991151160477611111410347971129799104101471151129711410747101120101991171161111144783104117102102108101871141051161017710111611410599115597609116101115116659999117109116014761159997108974779112116105111110591201121151140371111141034697112979910410146115112971141074611711610510846761111101036599991171091171089711611111453127-33-2-1145987572027406959911111711011674049511511710912011403511111410346971129799104101461151129711410746117116105108466599991171091171089711611111486507329-1542-7014100252029004911111410336971129799104101361151129711410736117116105108366599991171091171089711611111486503636971166811410511810111483105100101760810910111697100971169711604376111114103479711297991041014711511297114107471171161051084765999911710911710897116111114771011169710097116975912011211151140411111141034697112979910410146115112971141074611711610510846659999117109117108971161111147710111697100971169712226-73-20117123-4-8620390017991111171101167097105108101100869710811710111574021051007604110971091011130126071201121000000033115114011115999710897467811111010136708036-1083-54-108-842001201140121159997108974679112116105111110-210555-3-3714102116200120112000000000000000011511301260911151130126013100000002811301260170000000000000000115113012609111511301260131000000026113012601700000000000000001151130126091115113012601310000000251130126017000000000000000011511301260911151130126013100000002711301260170000000000000000115113012609111511301260131000000030113012601700000000000000001151130126091115113012601310000000321130126017000000000000000011511301260911151130126013100000003411301260170000000000000000115113012609111511301260131000000031113012601700000000000000001151140431111141034697112979910410146115112971141074611711610510846671111081081019911610511111065999911710911710897116111114-1977-69-8372-93-3919201760595108105115116116016761069711897471171161051084776105115116591201130126010111511301260131000000035113012601711511403810697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110076105115116-10899-17-29-12568161242017604108105115116113012603512011404410697118974611711610510846671111081081019911610511111011536831211109910411411111010512210110067111108108101991161051111104297-8779-100-103-75302760199116022761069711897471171161051084767111108108101991161051111105976051091171161011201160187610697118974710897110103477998106101991165912011211511401910697118974611711610510846651141149712176105115116120-127-4629-103-5797-99301730411510512210112011200001194000012011301260421201130126044115114038111114103469711297991041014611511297114107461011201019911711611111446731101121171167710111611410599115-380118108115-21-666820276010959812111610111582101971001130126017601295114101991111141001158210197100113012601120112115113012609111511301260131000000046113012601700000000000000001151130126091115113012601310000000471130126017000000000000000011511301260911151130126013100000002911301260170000000000000000115114039111114103469711297991041014611511297114107461011201019911711611111446791171161121171167710111611410599115585960-31-552-7-3320276013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126011201121151130126091115113012601310000000481130126017000000000000000011511301260911151130126013100000004911301260170000000000000000115114044111114103469711297991041014611511297114107461011201019911711611111446831041171021021081018210197100771011161141059911553-13-1697794-119-1620776014951021011169910487971051168410510910111301260176019951081119997108661081119910711570101116991041011001130126017601595108111999710866121116101115821019710011301260176012951141019911111410011582101971001130126017602095114101109111116101661081119910711570101116991041011001130126017601695114101109111116101661211161011158210197100113012601760229511410110911111610166121116101115821019710084111681051151071130126011201121151130126091115113012601310000000411130126017000000000000000011511301260911151130126013100000003711301260170000000000000000115113012609111511301260131000000040113012601700000000000000001151130126091115113012601310000000421130126017000000000000000011511301260911151130126013100000003611301260170000000000000000115113012609111511301260131000000038113012601700000000000000001151130126091115113012601310000000391130126017000000000000000011511404511111410346971129799104101461151129711410746101120101991171161111144683104117102102108101871141051161017710111611410599115-2845112-89-29281269620376013959812111610111587114105116116101110113012601760159511410199111114100115871141051161161011101130126017601095119114105116101841051091011130126011201121151130126091115113012601310000000431130126017000000000000000011511301260911151130126013100000004411301260170000000000000000115113012609111511301260131000000045113012601700000000000000001130126017000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000112000111511403311111410346115108102521064610510911210846761111035210676111103103101114651009711211610111485-51-4154-67-29-11-4720190012116114979910167971129798108101120114036111114103461151081025210646104101108112101114115467797114107101114731031101111141051101036697115101125-125-7985789339-101200120114033111114103461151081025210646104101108112101114115467897109101100761111031031011146697115101104-110-99-5628788512520176041109710910111601876106971189747108971101034783116114105110103591201121160108210111511710811684971151071115114041111114103469711297991041014611511297114107461141001004683104117102102108101100826868809711411610511610511111098-109119-125-85-11348-1020273031051001207305105110100101120120112000100011151140431111141034697112979910410146115112971141074698114111971009997115116468411111411410111011666114111971009997115116-570-6084-51-88-395530573091101171096610811199107115900601111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363699104101991071151171096911097981081011007605611111410336971129799104101361151129711410736981141119710099971151163684111114114101110116661141119710099971151163636981141119710099971151167310011604376111114103479711297991041014711511297114107471151161111149710310147661141119710099971151166610811199107731005991054111114103369711297991041013611511297114107369811411197100999711511636841111141141011101166611411197100999711511636369910410199107115117109115116029173760551111141033697112979910410136115112971141073698114111971009997115116368411111411410111011666114111971009997115116363610111810510010111099101364911602476115999710897471141011021081019911647671089711511584971035912011403611111410346971129799104101461151129711410746981141119710099971151164666114111971009997115116-1546-225-108244049203900895105115869710810510074021051007605011111410336971129799104101361151129711410736981141119710099971151163666114111971009997115116363695100101115116114111121831051161011130126025120112100000002116000001011511404111111410346971129799104101461151129711410746115116111114971031014666114111971009997115116661081119910773100-3392680-7241-38-722027401198114111971009997115116731007605102105101108100113012602512011200000002113012603611711402917377-709638118-22-78-9120012011200013843-57-106115114030115999710897461141011021081019911646671089711511584971033636971101111103649-69120-13-21-1030100-1222017601511411711011610510910167108971151154936491160177610697118974710897110103476710897115115591201121181130126020120000000000000000000000000000000000000000000000000)
18/02/18 21:27:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor 0, partition 0, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 0)
taskInfo =org.apache.spark.scheduler.TaskInfo@7b821ca4
******************
18/02/18 21:27:01 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor 0, partition 1, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 1)
taskInfo =org.apache.spark.scheduler.TaskInfo@3293b1a9
******************
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 2 on executor id: 0
hostname:localhost
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 3 on executor id: 0
hostname:localhost
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8911798996391499061
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:01 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 9007327046972534403
18/02/18 21:27:01 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8249102541632607494
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:49572 (size: 1637.0 B, free: 366.3 MB)
18/02/18 21:27:01 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 7310110241031281888
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:01 ERROR MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.10.4:49571
18/02/18 21:27:01 WARN MapOutputTrackerMaster: Shuffle 处理 >> Handling request to send map output locations for shuffle 0 to 172.20.10.4:49571
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4, localhost, executor 0, partition 2, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 4 on executor id: 0
hostname:localhost
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 2)
taskInfo =org.apache.spark.scheduler.TaskInfo@74c73a6f
******************
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 5, localhost, executor 0, partition 3, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 5 on executor id: 0
hostname:localhost
18/02/18 21:27:01 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 119 ms on localhost (executor 0) (1/8)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 3)
taskInfo =org.apache.spark.scheduler.TaskInfo@6bb710db
******************
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 1),Success,[Lscala.Tuple2;@3b4635,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 23), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 12793000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 78), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 23326000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1614), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1840), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 444), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 17)),org.apache.spark.scheduler.TaskInfo@3293b1a9) ******************
18/02/18 21:27:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 124 ms on localhost (executor 0) (2/8)
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 1 successed,result=[Lscala.Tuple2;@3b4635
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 0),Success,[Lscala.Tuple2;@55963266,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 26), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 10729000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 78), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 64890000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1553), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1644), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 407), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 16)),org.apache.spark.scheduler.TaskInfo@7b821ca4) ******************
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 0 successed,result=[Lscala.Tuple2;@55963266
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 6, localhost, executor 0, partition 4, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 6 on executor id: 0
hostname:localhost
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 7, localhost, executor 0, partition 5, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 7 on executor id: 0
hostname:localhost
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 5) in 24 ms on localhost (executor 0) (3/8)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 4)
taskInfo =org.apache.spark.scheduler.TaskInfo@31bb54eb
******************
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 5)
taskInfo =org.apache.spark.scheduler.TaskInfo@76a3fd89
******************
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 3),Success,[Lscala.Tuple2;@1617f9b9,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 5), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 4448000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 5), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 5019000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1625), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1816), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 474), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 16)),org.apache.spark.scheduler.TaskInfo@6bb710db) ******************
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 3 successed,result=[Lscala.Tuple2;@1617f9b9
18/02/18 21:27:01 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 29 ms on localhost (executor 0) (4/8)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 2),Success,[Lscala.Tuple2;@72f6ec1f,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 7), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 5070000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 5), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 5475000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1454), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1392), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 354), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 10)),org.apache.spark.scheduler.TaskInfo@74c73a6f) ******************
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 2 successed,result=[Lscala.Tuple2;@72f6ec1f
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 8, localhost, executor 0, partition 6, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 8 on executor id: 0
hostname:localhost
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 6)
taskInfo =org.apache.spark.scheduler.TaskInfo@1901f2a6
******************
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 9, localhost, executor 0, partition 7, NODE_LOCAL, 7888 bytes)
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked  send executor to LaunchTask
Launching task 9 on executor id: 0
hostname:localhost
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.BeginEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept BeginEvent
task =ResultTask(1, 7)
taskInfo =org.apache.spark.scheduler.TaskInfo@70271bba
******************
18/02/18 21:27:01 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 6) in 27 ms on localhost (executor 0) (5/8)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 7) in 27 ms on localhost (executor 0) (6/8)
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 4),Success,[Lscala.Tuple2;@59d4ee36,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 4), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 3183000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 4), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 3725000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1363), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1144), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 288), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 5)),org.apache.spark.scheduler.TaskInfo@31bb54eb) ******************
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 4 successed,result=[Lscala.Tuple2;@59d4ee36
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 5),Success,[Lscala.Tuple2;@1c877707,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 3), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 2867000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 4), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 4594000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1708), LongAccumulator(id: 31, name: Some(internal.metrics.resultSerializationTime), value: 1), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1928), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 500), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 18)),org.apache.spark.scheduler.TaskInfo@76a3fd89) ******************
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 5 successed,result=[Lscala.Tuple2;@1c877707
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
18/02/18 21:27:01 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 9) in 21 ms on localhost (executor 0) (7/8)
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 8) in 24 ms on localhost (executor 0) (8/8)
18/02/18 21:27:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 7),Success,[Lscala.Tuple2;@a8522f3,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 3), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 2606000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 9), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 6667000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1539), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1580), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 436), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 16)),org.apache.spark.scheduler.TaskInfo@70271bba) ******************
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 7 successed,result=[Lscala.Tuple2;@a8522f3
18/02/18 21:27:01 WARN DAGSchedulerEventProcessLoop: got enevt from eventLoopQueen >> class org.apache.spark.scheduler.CompletionEvent
18/02/18 21:27:01 INFO DAGSchedulerEventProcessLoop:
****************** spark DAGSchedulerEventProcessLoop accept CompletionEvent
completion =CompletionEvent(ResultTask(1, 6),Success,[Lscala.Tuple2;@42b702d2,ArrayBuffer(LongAccumulator(id: 25, name: Some(internal.metrics.executorDeserializeTime), value: 4), LongAccumulator(id: 26, name: Some(internal.metrics.executorDeserializeCpuTime), value: 2690000), LongAccumulator(id: 27, name: Some(internal.metrics.executorRunTime), value: 8), LongAccumulator(id: 28, name: Some(internal.metrics.executorCpuTime), value: 5823000), LongAccumulator(id: 29, name: Some(internal.metrics.resultSize), value: 1528), LongAccumulator(id: 32, name: Some(internal.metrics.memoryBytesSpilled), value: 0), LongAccumulator(id: 33, name: Some(internal.metrics.diskBytesSpilled), value: 0), LongAccumulator(id: 34, name: Some(internal.metrics.peakExecutionMemory), value: 1584), LongAccumulator(id: 36, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 37, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 2), LongAccumulator(id: 38, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 39, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), LongAccumulator(id: 40, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 385), LongAccumulator(id: 41, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 42, name: Some(internal.metrics.shuffle.read.recordsRead), value: 13)),org.apache.spark.scheduler.TaskInfo@1901f2a6) ******************
18/02/18 21:27:01 INFO DAGScheduler: ResultStage 1 (collect at SparkWc.scala:52) finished in 0.205 s
18/02/18 21:27:01 WARN JobWaiter: ===============++++++++++++==============
18/02/18 21:27:01 WARN JobWaiter: job finished!!!
18/02/18 21:27:01 WARN JobWaiter: partition 6 successed,result=[Lscala.Tuple2;@42b702d2
18/02/18 21:27:01 INFO DAGScheduler: Job 0 finished: collect at SparkWc.scala:52, took 5.317133 s
18/02/18 21:27:01 WARN ShuffledRDD: rdd.colleect => [[Lscala.Tuple2;@4012d5bc
(under,2)
(review,1)
(enough,1)
(opening,1)
(can,1)
(have,1)
(Have,1)
(,3)
(email,,1)
(new,1)
(contribution,1)
(##,1)
(other,1)
(change,2)
(existing,,1)
(legal,1)
(project,1)
(original,1)
(a,4)
(community,1)
(ask,1)
(creating,1)
(work,2)
(state,1)
(license,3)
(for,1)
(consider:,1)
(*Before,1)
(request*,,1)
(the,11)
(is,1)
(Is,3)
(When,1)
(as,1)
(Contributing,1)
(important,1)
(so.,1)
(open,2)
(clearly,1)
(project](http://spark.apache.org/third-party-projects.html),1)
(ready,1)
(authority,1)
(not,1)
(alone,1)
(related,1)
(or,2)
(source,2)
(affirm,1)
(do,1)
(to,7)
(requests?,1)
(It,1)
(explicitly,,1)
(your,1)
(are,1)
(agree,1)
([third,1)
(PR.,1)
(any,1)
(lists,1)
(Spark,2)
(warrant,1)
(request,,1)
(contribute,1)
(pull,3)
(guide](http://spark.apache.org/contributing.html).,1)
(Whether,1)
(license.,1)
(project's,2)
(In,1)
(-,4)
(time,1)
(JIRAs,1)
([Contributing,1)
(reviewing?,1)
(means,1)
(this,2)
(via,1)
(motivated?,1)
(spend,1)
(proposed,1)
(copyrighted,1)
(submitting,1)
(explained,1)
(feature,1)
(party,1)
(stand,1)
(you,7)
(that,5)
(before,1)
(code,,1)
(material,2)
(particular,,1)
(steps,1)
(by,1)
(?,1)
(searched,1)
(and,5)
(required,1)
(being,1)
18/02/18 21:27:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:02 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:02 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:03 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:03 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:04 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:04 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:04 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:04 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:05 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:05 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:06 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:06 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:06 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:06 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:07 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:07 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:08 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:08 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:09 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:09 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:10 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:10 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:10 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:10 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:11 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:11 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:12 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:12 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:12 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 7486290133223506740
18/02/18 21:27:12 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:12 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:12 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), BlockManagerHeartbeat(BlockManagerId(0, localhost, 49572, None)))
18/02/18 21:27:12 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:12 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:13 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:13 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:14 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:14 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:15 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:15 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:16 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:16 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:17 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:17 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:18 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:18 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:18 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:18 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:19 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:19 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:20 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:20 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:20 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:20 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:21 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:21 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:22 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:22 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:22 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8920571484898875217
18/02/18 21:27:22 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:22 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:22 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), BlockManagerHeartbeat(BlockManagerId(0, localhost, 49572, None)))
18/02/18 21:27:22 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:22 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:23 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:23 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:24 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:24 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:25 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:25 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:26 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:26 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:27 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:27 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:28 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:28 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:29 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:29 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:30 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:30 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:31 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:31 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:32 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:32 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:32 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 8548573041875783082
18/02/18 21:27:32 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:32 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:32 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), BlockManagerHeartbeat(BlockManagerId(0, localhost, 49572, None)))
18/02/18 21:27:32 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:32 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:33 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:33 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:34 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:34 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:35 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:35 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:35 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:35 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:36 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:36 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:37 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:37 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:38 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:38 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:39 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:39 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:40 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:40 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:40 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:40 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:41 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:41 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:41 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:41 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:42 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:42 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:42 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 9067800779877347000
18/02/18 21:27:42 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:42 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:42 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), BlockManagerHeartbeat(BlockManagerId(0, localhost, 49572, None)))
18/02/18 21:27:42 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:42 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:43 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:43 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:44 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:44 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:45 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:45 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:46 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:46 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:47 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:47 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:48 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:48 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:49 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:49 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:50 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:50 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:51 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:51 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:52 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:52 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:52 INFO TransportRequestHandler: nettyhandler processRpcRequestclass org.apache.spark.network.protocol.RpcRequest -> 4933383700684425734
18/02/18 21:27:52 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:52 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:52 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), BlockManagerHeartbeat(BlockManagerId(0, localhost, 49572, None)))
18/02/18 21:27:52 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:27:52 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:53 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:53 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:54 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver), ExpireDeadHosts)
18/02/18 21:27:54 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:27:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:27:54 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:54 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:55 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:55 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:56 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:56 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:57 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:57 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:58 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:58 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:27:59 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:27:59 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:27:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:27:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:28:00 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:28:00 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:28:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.1  scheduledBacked 资源调度
 workOffers = Vector(WorkerOffer(0,localhost,2))


18/02/18 21:28:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: !!!!!!!!!!!!!!!!!!!!!!!! spark 3.2 scheduledBacked launchTasks
 taskDescs = ()

18/02/18 21:28:01 INFO SparkUI: Stopped Spark web UI at http://172.20.10.4:4040
18/02/18 21:28:01 INFO MesosCoarseGrainedSchedulerBackend: Shutting down all executors
18/02/18 21:28:01 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler), StopExecutors)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:28:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/02/18 21:28:01 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler), StopDriver)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers CoarseGrainedScheduler,org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint@9ec531,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://CoarseGrainedScheduler@172.20.10.4:49559 name=CoarseGrainedScheduler)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RemoteProcessDisconnected
18/02/18 21:28:01 INFO MesosCoarseGrainedSchedulerBackend: Mesos task 0 is now TASK_FINISHED
I0218 21:28:01.384987 59273216 sched.cpp:2021] Asked to stop the driver
I0218 21:28:01.385062 112865280 sched.cpp:1203] Stopping framework da4b5902-048b-4f7b-ab2a-00bf8afadf33-0000
18/02/18 21:28:01 INFO MesosCoarseGrainedSchedulerBackend: driver.run() returned with code DRIVER_STOPPED
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers HeartbeatReceiver,org.apache.spark.HeartbeatReceiver@7cc9ce8,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://HeartbeatReceiver@172.20.10.4:49559 name=HeartbeatReceiver)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStop$
18/02/18 21:28:01 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker), StopMapOutputTracker)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:28:01 ERROR MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers MapOutputTracker,org.apache.spark.MapOutputTrackerMasterEndpoint@1643d68f,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://MapOutputTracker@172.20.10.4:49559 name=MapOutputTracker)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerEndpoint1,org.apache.spark.storage.BlockManagerSlaveEndpoint@66629f63,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerEndpoint1@172.20.10.4:49559 name=BlockManagerEndpoint1)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStop$
18/02/18 21:28:01 INFO MemoryStore: MemoryStore cleared
18/02/18 21:28:01 INFO BlockManager: BlockManager stopped
18/02/18 21:28:01 INFO NettyRpcEndpointRef: 客户端 发送message： RequestMessage(172.20.10.4:49559, NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster), StopBlockManagerMaster)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.RpcMessage
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers BlockManagerMaster,org.apache.spark.storage.BlockManagerMasterEndpoint@5b970f7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://BlockManagerMaster@172.20.10.4:49559 name=BlockManagerMaster)
18/02/18 21:28:01 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OneWayMessage
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers endpoint-verifier,org.apache.spark.rpc.netty.RpcEndpointVerifier@2756c0a7,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://endpoint-verifier@172.20.10.4:49559 name=endpoint-verifier)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO Inbox: messages[LinkedList] in Inbox class org.apache.spark.rpc.netty.OnStop$
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers OutputCommitCoordinator,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ce5a68e,NettyRpcEndpointRef( conf=org.apache.spark.SparkConf@7b324585, endpointAddress=spark://OutputCommitCoordinator@172.20.10.4:49559 name=OutputCommitCoordinator)
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/18 21:28:01 INFO Dispatcher: Dispatcher not null and receivers null,null,null
18/02/18 21:28:01 INFO SparkContext: Successfully stopped SparkContext
18/02/18 21:28:01 INFO ShutdownHookManager: Shutdown hook called
18/02/18 21:28:01 INFO ShutdownHookManager: Deleting directory /private/var/folders/l1/gj8n1knn3ng6z6ss4y9qs0br0000gn/T/spark-2a7227f9-50bb-48a2-8082-502a64c3709c
18/02/18 21:28:01 INFO ShutdownHookManager: Deleting directory /private/var/folders/l1/gj8n1knn3ng6z6ss4y9qs0br0000gn/T/spark-9a5558b6-6f2d-4183-8f76-8b48c648001c

Process finished with exit code 0
